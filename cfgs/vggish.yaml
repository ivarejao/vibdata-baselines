# Experiment overall settings
epochs: 80
batch_size: 64
seed: 42
model_checkpoint_gap: 1
# Training settings
optimizer:
  name: Adam
  parameters:
    # lr: 0.0003
    lr: 0.001
    weight_decay: 0.001
# Define the learning rates schedules that will be used.
# Can be 1 or more schedulers
lr_scheduler:
  - name: ExponentialLR
    parameters:
      gamma: 0.9
model:
  name: vggish
  output_param: out_channel
  parameters:
    out_channel: 4
    softmax: True
dataset:
  name: CWRU
  groups_dir: "data/groups"
  raw: 
    root: "data/raw_datasets"
  deep:
    root: "data/deep_datasets"
    transforms:
      # - name: FilterByValue
      #   parameters:
      #     on_field: sample_rate
      #     values: 12000
      - name: SplitSampleRate
        parameters:
          on_field: signal
      - name: StandardScaler
        parameters: 
          on_field: signal
      - name: Spectrogram
        parameters:
          mode: "psd"
    # resize: True
    preprocess: True
    # postprocess: True
params_grid:
  lr: [0.01, 0.001, 0.0003]

