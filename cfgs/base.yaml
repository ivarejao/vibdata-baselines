# Experiment overall settings
epochs: 100
batch_size: 64
train_split: 10
seed: 42
# Run name can be customized to include others metadata
run_name: "{dataset}-{model}"
# Trainer settings, this is usually used to set the device in multiple gpus machines
# trainer:
#  devices: [1]
#  accelerator: "gpu"
precision: "bf16-mixed"
num_workers: 10
# Training settings
optimizer:
  name: Adam
  parameters:
    lr: 0.0003
    weight_decay: 0.001
lr_scheduler:
  name: ExponentialLR
  parameters:
    gamma: 0.95
model:
  name: xresnet18
  output_param: c_out
  parameters:
    c_in: 1
dataset:
  name: CWRU
  groups_dir: "data/groups"
  raw:
    root: "data/raw_datasets"
  deep:
    root: "data/deep_datasets"
    transforms:
      - name: SplitSampleRate
        parameters:
          on_field: signal
      # In case theres a need to filter by sample rate,
      # the filter should be inserted at this point
      # - name: FilterSampleRate
      #   parameters:
      #     sample_rate: X
      # If the experiment needs to normalize the sample rate,
      # the following transform should be used
      # - name: NormalizeSampleRate
      #  parameters:
      #    sample_rate: 97656
